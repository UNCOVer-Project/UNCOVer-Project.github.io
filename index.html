<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
	<link rel="stylesheet" href="style.css">
	<title>UNCOVer</title>
</head>
<body>
	<nav class="navbar sticky-top navbar-expand-lg navbar-dark bg-dark">
		<a class="navbar-brand" href="index.html">UNCOVer</a>
		<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
			<span class="navbar-toggler-icon"></span>
		</button>

		<div class="collapse navbar-collapse" id="navbarSupportedContent">
			<ul class="navbar-nav mr-auto">
				<li class="nav-item dropdown active">
					<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Idea
					</a>
					<div class="dropdown-menu" aria-labelledby="navbarDropdown">
						<a class="dropdown-item" href="#intro">Introduction</a>
						<a class="dropdown-item" href="#howUWork">How UNCOVer Works</a>
						<a class="dropdown-item" href="#tools">The Tools</a>
						<a class="dropdown-item" href="#demo">Demo</a>
					</div>
				</li>
				<li class="nav-item dropdown">
					<a class="nav-link dropdown-toggle" href="documentation-root.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Documentation
					</a>
					<div class="dropdown-menu" aria-labelledby="navbarDropdown">
						<a class="dropdown-item" href="documentation-root.html#introduction">Introduction</a>
						<a class="dropdown-item" href="documentation-root.html#preq">Prerequisites Functions</a>
					</div>
				</li>
			</ul>
		</div>
	</nav>

	<div class="container">
		<section id="intro">
			<div class="heading dark">
				<h2>UNCOVer : UNsighted COmputer Vision</h2>
			</div>
			<div class="card">
				<h4 class="heading" style="margin-bottom: 20px">Introduction</h4>
				<iframe width="auto" height="600" src="https://www.youtube.com/embed/tQOfxX1UuGs" class="embed-responsive-item" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				<div id="introParagraph" class="container" style="padding: 10px;">
					<p>
						UNCOVer is an accessibility tool for the sightless person that is worn like sunglasses. UNCOVer will use the technology of Artificial Intelligence to aid the user in identifying and locating objects and texts accurately.
					</p>
					<p>
						UNCOVer differentiates itself from the currently available products by offering multiple object detection complete with object name and location, powerful and precise object description by detecting the object pointed by the user's finger, and optical character recognition that enables the user to "hear" the characters, all in a single, easy-to-use package, and reasonable price within reach of many people.
					</p>
					<div class="card">
						<div class="card-body">
							<img src="https://raw.githubusercontent.com/UNCOVer-Project/UNCOVer-Main/common/imgs/UNCOVer%20-%20Components.jpg" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
						</div>
					</div>
					<p>
						UNCOVer core components consists of a single camera positioned to mimic user's field of view. This camera will be used to capture an image of the objects and texts to be analysed. To facilitate user with easy and simple to use interface, UNCOVer will feature a microphone powered by speech recognition so that the user can give command directly by speaking without pressing any button. All information will be given to the user as speech via the provided earphone.
					</p>
					<p>
						UNCOVer's prototype will use a single Raspberry Pi 3 Model B to handle and process the information. To detect object and recognise characters, UNCOVer will be powered by Azure Cognitive Service for reliable object & character recognition and speech services to deliver the best possible experience.
					</p>
					<p>
						To make sure blind people are within reach of the device, UNCOVer will be priced in a relatively low cost between 100 to 150 USD. The UNCOVer will be distributed to local medical vendors to reach the users.
					</p>
					<p>
						With all of those feaures, it is hoped that UNCOVer will give blind people independence in identifying and locating object, and reading text so they can enjoy life as much as normal people, making them <strong>uncover</strong> the countless information of the world.
					</p>
				</div>
			</div>
		</section>
		<section id="howUWork" style="padding-top:70px;">
			<div class="card">
				<h4 class="heading" style="margin-bottom: 20px">How UNCOVer Works</h4>
				<div class="card">
					<div class="card-body">
						<img src="https://raw.githubusercontent.com/UNCOVer-Project/UNCOVer-Main/common/imgs/UNCOVer%20flowchart.jpg" width=700 alt="Responsive image" class="img-fluid rounded mx-auto d-block">
						<h6>General Flowchart on How UNCOVer Works</h6>
					</div>
				</div>
				<div id="howUWorkParagraph" class="container">
					<p>
						When the device is turned on, the device will enter an initial state where the microphone will continuously record sound. The software in the raspberry pi that communicates with Azure Speech SDK will constantly perform speech recognition on the recorded stream of sound. If a sentence is completed, the software will then match the sentence in the recognised speech with the sentence of the available commands.
					</p>
					<p>
						If the sentence "What's in front of me" is found within the recognised speech, the camera on the device will capture an image in the direction where the user is facing. The software will then send the image to Azure's Computer Vision API. The API will perform finger detection which searches the user's finger on the image, and then return the information of the finger's location and orientation in the image if a finger is detected, otherwise the API will return without the information.
					</p>
					<p>
						UNCOVer will perform the task of detailed object recognition of the pointed object if a finger is detected. First, the software will crop the image based on the finger orientation to discard unwanted areas and minimise the search area. The finger location will be saved as a reference point for the later process. The cropped image will be sent to the Computer Vision API that will perform object recognition. The API will return complete information of the object(s) detected. If there is no object detected, then UNCOVer will say a message "No object is detected" and return to initial state. Else, the program will start calculating the distance from each detected object to the reference point. UNCOVer will pick the nearest object from the reference point and say the complete description of the chosen object, which is the pointed object.
					</p>
					<p>
						The other task that UNCOVer will perform is general object recognition where a finger is not detected. The software will directly send the image to the Computer Vision API to recognise object(s) and will return information of object(s)'s name and position. If no objects are detected, UNCOVer will say "No object is detected". Otherwise, the device will say each object(s)'s name and relative position.
					</p>
					<p>
						Back to the recognised speech, if the sentence "Read Text in front of me" is found, the camera will capture an image in the direction where the user is facing. The software will send the image to the Computer Vision API to perform character recognition in the image. The API will then return the information containing the text. If there are no text or any characters in the image, the returned text information will be blank and the device will say "No text or character is detected". Otherwise, the device will say all detected text in the image.
					</p>
					<p>
						UNCOVer will use Azure Speech Service for text-to-speech conversion to give the best output speech for the user. When the object or text is detected and contained in the information returned by the API as described previously, the string in the information that will be spoken to the user will be parsed first, and then sent to the Speech Service for TTS request. The Speech Service will return the speech as a .wav audio file. UNCOVer will then say the information by playing the audio file on the earphone. For instances where no object or text is found, the pre-recorded audio file will be played instead.
					</p>
				</div>
			</div>
		</section>
		<section id="tools" style="padding-top:70px;">
			<div class="card">
				<h4 class="heading" style="margin-bottom: 20px">Software Tools that Empower UNCOVer Prototype</h4>
				<div class="card">
					<div class="card-body">
						<img  src="https://github.com/UNCOVer-Project/UNCOVer-Main/blob/common/imgs/HackmyLief-0002.jpg?raw=true" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
						<h6>Things that empower UNCOVer</h6>
					</div>
				</div>
				<div id="howUWorkParagraph" class="container">
					<h6>Azure Vision Cognive Services</h6>
					<p>
						Extract rich information from images to categorize and process visual data—and perform machine‑assisted moderaon of images.
					</p>
					<h6>Azure Speech Services</h6>
					<p>
						Swily convert audio to text for natural responsiveness. The Speech to Text and Text to Speech API is part of the Speech services.
					</p>
					<h6>Python</h6>
					<p>
						Python is used for simplicity, versality, and cross‑plaorm compability. Azure Cognive Services supports Python.
					</p>
					<h6>Raspbian OS</h6>
					<p>
						Raspbian OS is the default OS of Raspberry Pi. Raspbian is based on linux. The OS will oﬀer ﬂexibility in developing the software.
					</p>
				</div>
			</div>
		</section>
		<section id="demo" style="padding-top:70px;">
			<div class="card">
				<h4 class="heading" style="margin-bottom: 20px">Demo of UNCOVer in Action</h4>
				<iframe width="auto" height="600" src="https://www.youtube.com/embed/ToJfDamJIUE" class="embed-responsive-item" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				<div id="demoParagraph" class="container" style="padding: 10px;">
					<p>
						This video is a demonstration clip of the UNCOVer device.
					</p>
					<p>
						UNCOVer is an accessibility tool for the sightless person that is worn like sunglasses. UNCOVer will use the technology of Artificial Intelligence to aid the user in identifying and locating objects and texts accurately.
					</p>
					<p>
						UNCOVer differentiates itself from the currently available products by offering multiple object detection complete with object name and location, powerful and precise object description by detecting the object pointed by the user's finger, and optical character recognition that enables the user to "hear" the characters, all in a single, easy-to-use package, and reasonable price within reach of many people.
					</p>
					<p>
						This video is divided into 5 parts of demonstration:
						<ul>
							<li>
								Part 1: Object Detection with object's name and relative location		
							</li>
							<li>
								Part 2: Describe Pointed Object
							</li>
							<li>
								Part 3: Text Reading from Image
							</li>
							<li>
								Part 4: Analyzing context from an Image
							</li>
							<li>
								Part 5: Describing Image
							</li>
						</ul>	
							
					</p>
				</div>
			</div>
		</section>
	</div>

	<!-- Javascripts-->
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
	<script>
		var sections = $('section'),
		nav = $('nav'),
		nav_height = nav.outerHeight();
	
		$(window).on('scroll', function () {
			  var cur_pos = $(this).scrollTop();
	
			  sections.each(function() {
				var top = $(this).offset().top - nav_height,
				bottom = top + $(this).outerHeight();
	
				if (cur_pos >= top && cur_pos <= bottom) {
					  nav.find('a').removeClass('active');
					  sections.removeClass('active');
	
				  $(this).addClass('active');
					  nav.find('a[href="#'+$(this).attr('id')+'"]').addClass('active');
				}
			  });
		});
		</script>
</body>
</html>