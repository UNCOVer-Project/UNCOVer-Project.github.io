<!DOCTYPE html>
<html style="scroll-behavior: smooth;">
<head>
	<title>UNCOVer : UNsighted COmputer Vision</title>
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<<<<<<< HEAD
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

=======
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
>>>>>>> First commit
</head>
<body style="align-items: center;">
	<nav class="navbar sticky-top navbar-expand-lg navbar-dark bg-dark">
		<a class="navbar-brand" href="#">UNCOVer</a>
			<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
				<span class="navbar-toggler-icon"></span>
			</button>

		<div class="collapse navbar-collapse" id="navbarSupportedContent">
			<ul class="navbar-nav mr-auto">
			  <li class="nav-item" id="backgroundNav">
			    <a class="nav-link" href="#background">Background</a>
			  </li>
			  <li class="nav-item" id="whatisNav">
<<<<<<< HEAD
			    <a class="nav-link" href="#whatis">What Is Unblinded </a>
			  </li>
			  <li class="nav-item" id="howitNav">
			    <a class="nav-link" href="#howit">How It Works </a>
			  </li>
				<li class="nav-item" id="thingsweNav">
			    <a class="nav-link" href="#thingsweuse">The Pillars</a>
=======
			    <a class="nav-link" href="#whatis">What Is UNCOVer </a>
			  </li>
			  <li class="nav-item" id="howitNav">
			    <a class="nav-link" href="#howit">How To Use UNCOVer </a>
			  </li>
		 		<li class="nav-item" id="howUWorkNav">
			    <a class="nav-link" href="#howUWork">How UNCOVer Works</a>
			  </li>
			  <li class="nav-item" id="toolsNav">
			    <a class="nav-link" href="#tools">The Tools</a>
>>>>>>> First commit
			  </li>
				<li class="nav-item" id="aboutUsNav">
			    <a class="nav-link" href="#aboutus">About us</a>
			  </li>
			</ul>
		</div>
	</nav>

	<div class="container" style="padding: 20px">
		<section id="background" style="padding-top:10px;">
				<div class="heading dark" style="margin-bottom:20px">
					<h2>UNCOVer : UNsighted COmputer Vision</h2>
				</div>
			<div class="card" style="padding: 20px; margin-bottom: 20px">
				<h4 class="heading" style="margin-bottom: 20px">Background</h4>
<<<<<<< HEAD
				<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/Blind-Person-Enjoying-Outdoors.jpg" class="img-fluid rounded mx-auto d-block" alt="Responsive image">
				<div id="backgroundParagraph" class="container" style="padding: 10px">
					<p style="text-align: justify; margin-bottom: 20px">
					<mark style="background-color: #fcee53;">A sightless person</mark> has difficulties in identifying objects. They sense their <mark style="background-color: #fcee53;">surroundings</mark> and know the <mark style="background-color: #fcee53;">presence of objects</mark> with their <mark style="background-color: #fcee53;">remaining (upgraded) perceptions</mark>.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						But, with all of their adaptation, there is still one differentiating factor that makes them <mark style="background-color: #fcee53;">less precise</mark> in detecting object, that is <mark style="background-color: #fcee53;">finding</mark> and <mark style="background-color: #fcee53;">locating</mark> small sized and complex shaped object.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						In general scenario, a sightless person is very dependant in memory and their senses. They can quickly identify objects that are <mark style="background-color: #fcee53;">routinely used and remember</mark>, but may have difficulties <mark style="background-color: #fcee53;">identifying completely new object in different or unusual locations</mark>.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Based on this problem, we offer a solution that we named <mark style="background-color: #fcee53;">UNCOVer</mark>.
=======
				<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/Blind-Person-Enjoying-Outdoors.jpg" class="img-fluid rounded mx-auto d-block" alt="Responsive image" style="margin-bottom:20px;">
				<div id="backgroundParagraph" class="container" style="padding: 10px;">
					<p style="text-align: justify; margin-bottom: 20px">
					Blind people are people who have complete or nearly complete vision loss and may result in difficulties in daily life routines such as walking, socialising and reading. As of 2015 there were 940 million people with some degree of vision loss (Collaborators 2015). Blind people do lead a normal life with their own style of doing things. But, they definitely face troubles due to inaccessible infrastructure and social challenges.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						According to Erin Brady et al., the most common accessibility issues encountered in everyday life is Identification, making up 41% of the total sample. More than half of Identification questions were simple, “No Context” questions (58%) where the user did not provide any information about the subject of the object (Brady et al. 2013).
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Other problems that is faced by blind people are locating an object. Popular culture depicts that if one of the senses of a person stops working, the others become sharper. Blind people may rely more on their other senses and develop a strong memory, but this may not be well enough to identify object and their location. Blind people must memorise the location of every obstacle or item in their home environment. Any changes in location can put burden to the person to search the item again with varying difficulties.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Other problems that is faced by blind people are locating an object. Popular culture depicts that if one of the senses of a person stops working, the others become sharper. Blind people may rely more on their other senses and develop a strong memory, but this may not be well enough to identify object and their location. Blind people must memorise the location of every obstacle or item in their home environment. Any changes in location can put burden to the person to search the item again with varying difficulties.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Despite the challenges faced by the blind, the most valuable thing for a disabled person is gaining independence. A blind person can lead an independent life with some specifically designed adaptive things for them. There are lots of adaptive equipment that can enable a blind person to live their life independently but they are not easily available in the local shops or markets. They have to put much effort to get each equipment that can take them one step closer towards independence (Kumar 2018).
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Computers and the Internet are two of the most significant developments since the invention of braille, as for the first time ever many blinds and partially sighted people have access to the same wealth of information as sighted people and on the same terms. Computing technology is promising greater accessibility to information, services, and society (Jarry et al. 2017).
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						With the premise above, we came up with a powerful and easy-to-use solution for the blind to identify and discover the location of objects that we called <mark style="background-color: #fcee53;">UNCOVer: UNsighted COmputer Vision</mark>.
>>>>>>> First commit
					</p>
				</div>
			</div>
		</section>
		<section id="whatis" style="padding-top:70px;">
			<div class="card" style="padding: 20px; margin-bottom: 20px">
				<h4 class="heading" style="margin-bottom: 20px">What is UNCOVer</h4>
				<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/UNBLINDED%20concept.png" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
<<<<<<< HEAD
				<div id="backgroundParagraph" class="container" style="padding: 10px; ">
					<p style="text-align: justify; margin-bottom: 20px">
						<mark style="background-color: #fcee53;">UNCOVer</mark> is an accessibility tool that is used as a headwear. <mark style="background-color: #fcee53;">UNCOVer</mark> is intended for sightless person to help them <mark style="background-color: #fcee53;">identify objects</mark> in direction 	<mark style="background-color: #fcee53;">they are facing to</mark>. UNCOVer works by identifying the object that are inside of a captured image with 	<mark style="background-color: #fcee53;">Azure Vision</mark> and then outputs the object's name and location to the user.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						<mark style="background-color: #fcee53;">UNCOVer</mark> core component consists of <mark style="background-color: #fcee53;">a microphone</mark> to record user's voice command, <mark style="background-color: #fcee53;">a speaker</mark> to speak the object information to the user, <mark style="background-color: #fcee53;">a camera</mark> to capture an image in the direction of the user, and <mark style="background-color: #fcee53;">a microcontroller</mark> to process and manage the data.
=======
				<div id="whatisParagraph" class="container" style="padding: 10px; ">
					<p style="text-align: justify; margin-bottom: 20px">
						UNCOVer is an accessibility tool for the sightless person that is worn like sunglasses. UNCOVer will use the technology of Artificial Intelligence to aid the user in identifying and locating objects and texts accurately.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						UNCOVer differentiates itself from the currently available products by offering multiple object detection complete with object name and location, powerful and precise object description by detecting the object pointed by the user's finger, and optical character recognition that enables the user to "hear" the characters, all in a single, easy-to-use package, and reasonable price within reach of many people.
					</p>
					<div class="card" style="margin-bottom: 20px">
						<div class="card-body">
							<img src="https://raw.githubusercontent.com/UNCOVer-Project/UNCOVer-Main/common/HackMyLifeGraphic/UNCOVer%20-%20Components.jpg" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
						</div>
					</div>
					<p style="text-align: justify; margin-bottom: 20px">
						UNCOVer core components consists of a single camera positioned to mimic user's field of view. This camera will be used to capture an image of the objects and texts to be analysed. To facilitate user with easy and simple to use interface, UNCOVer will feature a microphone powered by speech recognition so that the user can give command directly by speaking without pressing any button. All information will be given to the user as speech via the provided earphone.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						UNCOVer's prototype will use a single Raspberry Pi 3 Model B to handle and process the information. To detect object and recognise characters, UNCOVer will be powered by Azure Cognitive Service for reliable object & character recognition and speech services to deliver the best possible experience.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						To make sure blind people are within reach of the device, UNCOVer will be priced in a relatively low cost between <mark style="background-color: #fcee53;">100 to 150 USD</mark>. The UNCOVer will be distributed to local medical vendors to reach the users.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						With all of those feaures, it is hoped that UNCOVer will give blind people independence in identifying and locating object, and reading text so they can enjoy life as much as normal people, making them <strong>uncover</strong> the countless information of the world.
>>>>>>> First commit
					</p>
				</div>
			</div>
		</section>
		<section id="howit" style="padding-top:70px;">
			<div class="card" style="padding: 20px; margin-bottom: 20px">
<<<<<<< HEAD
				<h4 class="heading" style="margin-bottom: 20px">How It Works</h4>
				<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/Unblinded%20Flowchart.jpg" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
				<div id="backgroundParagraph" class="container" style="padding: 10px; ">
					<p style="text-align: justify; margin-bottom: 20px">
						While the device is on, the microphone will <mark style="background-color: #fcee53;">constantly monitor for user's commands</mark>. The user can speak <mark style="background-color: #fcee53;">"What's in front of me"</mark> in order for UNCOVer to <mark style="background-color: #fcee53;">capture an image</mark>. After that, the image will be <mark style="background-color: #fcee53;">sent to Azure Vision API</mark> to be analyzed for individual objects. The API will then <mark style="background-color: #fcee53;">return</mark> information of <mark style="background-color: #fcee53;">object names</mark> and <mark style="background-color: #fcee53;">locations</mark>. If there are <mark style="background-color: #fcee53;">no object detected</mark>, then text-to-speech engine will output <mark style="background-color: #fcee53;">"There are no detected objects in front of you"</mark> through the speaker. If there are <mark style="background-color: #fcee53;">object(s) detected</mark>, then the device will be speaking <mark style="background-color: #fcee53;">each object(s) name and location</mark>. Alternatively, if a <mark style="background-color: #fcee53;">finger pointing to a specific object is detected</mark>, then the device will <mark style="background-color: #fcee53;">speak the pointed object's name</mark>. After that, the microphone will <mark style="background-color: #fcee53;">continue to monitor user commands</mark>. If the user say <mark style="background-color: #fcee53;">"Turn off"</mark>, then the device will <mark style="background-color: #fcee53;">stop monitoring</mark> and <mark style="background-color: #fcee53;">turn off</mark>. Else, the device will keep monitoring and work as described.
					</p>
=======
				<h4 class="heading" style="margin-bottom: 20px">How To Use UNCOVer</h4>
				<div id="howitParagraph" class="container" style="padding: 10px;">
					<div class="card" style="margin-bottom: 20px">
						<div class="card-body">
							<img  src="https://raw.githubusercontent.com/UNCOVer-Project/UNCOVer-Main/common/HackMyLifeGraphic/UNCOVer%20-%20HowItWorks-0001.jpg" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
							<h6 style="margin-bottom: 20px">Steps to use UNCOVer on object recognition</h6>
							<img  src="https://raw.githubusercontent.com/UNCOVer-Project/UNCOVer-Main/common/HackMyLifeGraphic/UNCOVer%20-%20HowItWorks-0002A.jpg" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
							<h6>Steps to use UNCOVer on text recognition</h6>
						</div>
					</div>
					<p style="text-align: justify; margin-bottom: 20px">
						Operating UNCOVer is very straightforward. To start using the device, the user must wear UNCOVer like a sunglasses. After the device has been firmly worn, he/she must turn on the device by pressing a button. After the device has been turned on, the user can now speak predetermined commands to UNCOVer.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						There are several commands that the user can speak of such as "What's in front of me" to identify objects in front of the user or to describe an object pointed by the user, "Read text in front of me" to recognise text in front of the user, and "Show me available commands" to show all available commands.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Lastly, if the user wishes to stop using the device, he/she have to turn off the device by pressing the same button used to turn on the device.
					</p>
					
>>>>>>> First commit
				</div>
			</div>
		</section>

<<<<<<< HEAD
		<section id="thingsweuse" style="padding-top:70px;">
			<div class="card" style="padding: 20px; margin-bottom: 20px">
				<h4 class="heading" style="margin-bottom: 20px">Project's Pillars</h4>
				<div id="backgroundParagraph" class="container" style="padding: 10px; ">
					<p style="text-align: justify; margin-bottom: 20px">
						In this section we will the describe the 'Pillars' of this program. So, here it is.
					</p>
				</div>
				<div class="row">
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="https://www.raspberrypi.org/magpi/wp-content/uploads/2016/10/RaspberryPi.jpg" class="img-fluid mx-auto d-block" style="width: auto;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Raspbian OS</h4>
						<p style="text-align: justify;">
							Raspbian OS is the default OS of Raspberry Pi, Raspbian is basically based on linux, so we can run our script more or less just like on our PC.
						</p>
					</div>
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSxKDtAIjDAQRtZ1YaZ6H_CDiUqooRT9NKn4pdDXvOFSFjNoU0dNw" class="img-fluid rounded mx-auto d-block" style="width: auto;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Python</h4>
						<p style="text-align: justify;">
							We use Python for simplicity, versatility, and cross-platform compatibility. Also, Azure services supports Python. So, yeah.
						</p>
					</div>
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="https://docs.microsoft.com/en-us/azure/media/index/api_computer_vision.svg" class="img-fluid rounded mx-auto d-block " style="width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Azure Vision Cognitive Services </h4>
						<p style="text-align: justify;">
							We use Azure Vision Cognitive Services for analyzing images we captured from the camera. With Azure's Vision and Custom Vision APIs, we can determine what objects are in front of the sightless person.
							It also gives the location of the item in the picture.
						</p>
					</div>
					<div class=" col-lg-3 " style="padding: 20px; margin-bottom: 20px; ">
						<img src="https://docs.microsoft.com/en-us/azure/media/index/api_bing_speech.svg" class="img-fluid rounded mx-auto d-block" style="width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Azure Speech Cognitive Services</h4>
						<p style="text-align: justify;">
							Using Azure's Speech Cognitive Services, we can translate user commands into text using Speech-to-Text service, so we can process user's request accordingly. The Text-To-Speech service is used to convert
							image analyze result (from a string), to a voice (or a speech) which later will be played to the user.
						</p>
					</div>
				</div>
				<div class="row">
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/1460800659967549.jpg" class="img-fluid mx-auto d-block" style="padding-top: 60px;padding-bottom: 60px;width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Raspberry Pi 3 Model B</h4>
						<p style="text-align: justify;">
							Raspberry Pi 3 Model B has everything we need for this project, which is a wi-fi module, 3.5 mm jack, USB Ports, and a lite Linux Distro made by Raspberry to run our program scripts. Raspberry Pi 3 Model B is fast enough to do everything we need.
						</p>
					</div>
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/178594_11ed128f-e115-47fd-ba45-f6c88e47d5be.jpg" class="img-fluid rounded mx-auto d-block" style="width: auto;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">USB Microphone</h4>
						<p style="text-align: justify;">
							This USB Microphone is plug and play on Raspberry Pi. It's used to listen for user's commands.
						</p>
					</div>
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/27923402_fac06b19-1087-48a3-9974-f4d99ea7eeea_700_700.jpg" class="img-fluid rounded mx-auto d-block " style="width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Battery</h4>
						<p style="text-align: justify;">
							We're designing a wearable tech. So it should be mobile, and it means battery. This battery and its controller are specifically designed for use with Raspberry 3 Model B.
						</p>
					</div>
					<div class=" col-lg-3 " style="padding: 20px; margin-bottom: 20px; ">
						<img src="https://raw.githubusercontent.com/agikarasugi/HackMyLife/master/HackMyLifeGraphic/81i9-Wm7f7L._SL1500_.jpg" class="img-fluid rounded mx-auto d-block" style="width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Camera</h4>
						<p style="text-align: justify;">
							The camera is used to take images for the software. From the picture of this camera, later it'll be processed using Azure Vision Cognitive Services and later on described through voice for the user.
						</p>
					</div>
					<div class="container" style="text-align:center;">
						<div class=" col-lg-3 " style="padding: 20px; margin-bottom: 20px; display:inline-block;">
							<img src="https://cdn.shopify.com/s/files/1/0966/5002/products/product-image-562838456.jpg?v=1528352990" class="img-fluid rounded mx-auto d-block" style="width: 100%;height: auto;vertical-align: middle;">
							<h4 style="text-align: center;">Earphone</h4>
							<p style="text-align: justify;">
								Rather than using speakers, we decided to use Earphone for the user to listen the output voice of the program. It helps voice insulation, the volume is adjustable, can be removed when not used, and cost-efficient.
								It's also applicable as an alternative for microphone.
							</p>
						</div>
						<div class=" col-lg-3 " style="margin-bottom: 20px; display:inline-block;">
							<img src="https://www.oakley.com/mam/celum/celum_assets/main_OO9102-01_holbrook_matte-black-warm-grey_001_63612_png_hero.jpg" class="img-fluid rounded mx-auto d-block" style="width: 100%;height: auto;vertical-align: middle;">
							<h4 style="text-align: center; padding-top: 65px;">Sunglasses</h4>
							<p style="text-align: justify;">
								Sunglasses is a common thing that sightless people use. So definitely we use it for this project.
							</p>
						</div>
					</div>
=======
		<section id="howUWork" style="padding-top:70px;">
			<div class="card" style="padding: 20px; margin-bottom: 20px">
				<h4 class="heading" style="margin-bottom: 20px">How UNCOVer Works</h4>
				<div class="card">
					<div class="card-body">
						<img src="https://raw.githubusercontent.com/UNCOVer-Project/UNCOVer-Main/common/HackMyLifeGraphic/UNCOVer%20flowchart.jpg" width=700 alt="Responsive image" class="img-fluid rounded mx-auto d-block">
						<h6>General Flowchart on How UNCOVer Works</h6>
					</div>
				</div>
				<div id="howUWorkParagraph" class="container" style="padding: 10px; ">
					<p style="text-align: justify; margin-bottom: 20px">
						When the device is turned on, the device will enter an initial state where the microphone will continuously record sound. The software in the raspberry pi that communicates with Azure Speech SDK will constantly perform speech recognition on the recorded stream of sound. If a sentence is completed, the software will then match the sentence in the recognised speech with the sentence of the available commands.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						If the sentence "What's in front of me" is found within the recognised speech, the camera on the device will capture an image in the direction where the user is facing. The software will then send the image to Azure's Computer Vision API. The API will perform finger detection which searches the user's finger on the image, and then return the information of the finger's location and orientation in the image if a finger is detected, otherwise the API will return without the information.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						UNCOVer will perform the task of detailed object recognition of the pointed object if a finger is detected. First, the software will crop the image based on the finger orientation to discard unwanted areas and minimise the search area. The finger location will be saved as a reference point for the later process. The cropped image will be sent to the Computer Vision API that will perform object recognition. The API will return complete information of the object(s) detected. If there is no object detected, then UNCOVer will say a message "No object is detected" and return to initial state. Else, the program will start calculating the distance from each detected object to the reference point. UNCOVer will pick the nearest object from the reference point and say the complete description of the chosen object, which is the pointed object.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						The other task that UNCOVer will perform is general object recognition where a finger is not detected. The software will directly send the image to the Computer Vision API to recognise object(s) and will return information of object(s)'s name and position. If no objects are detected, UNCOVer will say "No object is detected". Otherwise, the device will say each object(s)'s name and relative position.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						Back to the recognised speech, if the sentence "Read Text in front of me" is found, the camera will capture an image in the direction where the user is facing. The software will send the image to the Computer Vision API to perform character recognition in the image. The API will then return the information containing the text. If there are no text or any characters in the image, the returned text information will be blank and the device will say "No text or character is detected". Otherwise, the device will say all detected text in the image.
					</p>
					<p style="text-align: justify; margin-bottom: 20px">
						UNCOVer will use Azure Speech Service for text-to-speech conversion to give the best output speech for the user. When the object or text is detected and contained in the information returned by the API as described previously, the string in the information that will be spoken to the user will be parsed first, and then sent to the Speech Service for TTS request. The Speech Service will return the speech as a .wav audio file. UNCOVer will then say the information by playing the audio file on the earphone. For instances where no object or text is found, the pre-recorded audio file will be played instead.
					</p>
				</div>
			</div>
		</section>

		<section id="tools" style="padding-top:70px;">
			<div class="card" style="padding: 20px; margin-bottom: 20px">
				<h4 class="heading" style="margin-bottom: 20px">Tools That Empower UNCOVer Prototype</h4>
				<div class="card">
					<div class="card-body">
						<img  src="https://github.com/UNCOVer-Project/UNCOVer-Main/blob/common/HackMyLifeGraphic/HackmyLief-0002.jpg?raw=true" alt="Responsive image" class="img-fluid rounded mx-auto d-block">
						<h6>Things that empower UNCOVer</h6>
					</div>
				</div>
				<div id="howUWorkParagraph" class="container" style="padding: 10px; ">
					<h6>Azure Vision Cognive Services</h6>
					<p>
						Extract rich information from images to categorize and process visual data—and perform machine‑assisted moderaon of images.
					</p>
					<h6>Azure Speech Services</h6>
					<p>
						Swily convert audio to text for natural responsiveness. The Speech to Text and Text to Speech API is part of the Speech services.
					</p>
					<h6>Python</h6>
					<p>
						Python is used for simplicity, versality, and cross‑plaorm compability. Azure Cognive Services supports Python.
					</p>
					<h6>Raspbian OS</h6>
					<p>
						Raspbian OS is the default OS of Raspberry Pi. Raspbian is based on linux. The OS will oﬀer ﬂexibility in developing the software.
					</p>

>>>>>>> First commit
				</div>
			</div>
		</section>

<<<<<<< HEAD


		<!--  About us section-->
		<section id="aboutus" style="padding-top:70px;">
=======
		<!--  About us section-->
		<section id="aboutus" style="padding-top:70px; margin-bottom: 170px;">
>>>>>>> First commit
			<div class="card" style="padding: 20px; margin-bottom: 20px">
				<h4 class="heading" style="margin-bottom: 20px">About us</h4>
				<div id="aboutUsParagraph" class="container" style="padding: 10px; ">
					<p style="text-align: justify; margin-bottom: 20px">
						Our team biography that created UNCOVer concept and code
					</p>
				</div>

				<div class="row">
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="Img_Agi_Edited.jpg" class="img-fluid mx-auto d-block" style="width: auto;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">I Putu Agi <br>Karasugi</h4>
						<p style="text-align: justify; font-size: 14px; ">

							<br> Contact: <a href="mailto:i.karasugi@binus.ac.id"><br>i.karasugi@binus.ac.id</a>
							<br> Github: <a href="https://github.com/agikarasugi">https://github.com/agikarasugi</a>

						</p>
					</div>
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="Img_Mika.jpg" class="img-fluid rounded mx-auto d-block" style="width: auto;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Muhammad Teguh Pangestu</h4>
						<p style="text-align: justify; font-size: 14px; ">

							<br> Contact: <a href="mailto:muhammad.pangestu@binus.edu"><br>muhammad.pangestu@binus.edu</a>
							<br> Github: <a href="https://github.com/mstrassassin1st"><br>https://github.com/mstrassassin1st</a>


						</p>
					</div>
					<div class=" col-lg-3 " style=" padding: 20px; margin-bottom: 20px;">
						<img src="Img_Joshua.jpg" class="img-fluid rounded mx-auto d-block " style="width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Joshua Evan <br>Arijanto</h4>
						<p style="text-align: justify; font-size: 14px; ">

							<br> Contact: <a href="mailto:joshua.arijanto@binus.ac.id"><br>joshua.arijanto@binus.ac.id</a>
							<br> Github: <a href="https://github.com/JoshEvan"><br>https://github.com/JoshEvan</a>

						</p>
					</div>
					<div class=" col-lg-3 " style="padding: 20px; margin-bottom: 20px; ">
						<img src="Img_Adhito.jpeg" class="img-fluid rounded mx-auto d-block" style="width: 100%;height: auto;vertical-align: middle;">
						<h4 style="text-align: center;">Anindhito Irmandharu</h4>
						<p style="text-align: justify;  font-size: 14px; ">

							<br> Contact: <a href="mailto:Adhito909@gmail.com"><br>anindhito.irmandharu@binus.ac.id</a>
							<br> Github: <a href="https://github.com/Adhito"><br>https://github.com/Adhito</a>

						</p>
					</div>
				</div>
			</div>
		</section>
	<script>

	var sections = $('section'),
	nav = $('nav'),
	nav_height = nav.outerHeight();

	$(window).on('scroll', function () {
  		var cur_pos = $(this).scrollTop();

  		sections.each(function() {
    		var top = $(this).offset().top - nav_height,
        	bottom = top + $(this).outerHeight();

    		if (cur_pos >= top && cur_pos <= bottom) {
      			nav.find('a').removeClass('active');
      			sections.removeClass('active');

      		$(this).addClass('active');
      			nav.find('a[href="#'+$(this).attr('id')+'"]').addClass('active');
			}
  		});
	});
	</script>

</body>
</html>
